\documentclass{sice-si}

% タイトルと著者名
\title{視覚と行動のend-to-end学習により経路追従行動を\\
オンラインで模倣する手法の提案\\
-トポロジカルマップとシナリオに基づく経路選択機能の追加と検討-\\} % 和文タイトル
\name{○春山健太（千葉工大），藤原柾（千葉工大）馬場琉生（千葉工大）\\石黒巧（千葉工大）
上田隆一（千葉工大）林原靖男（千葉工大）} % 著者名
\etitle{A proposal for an online imitation method of
path-tracking behavior by end-to-end learning of vision and action\\
- Adding and consideration  of path selection function \\based on a topological map and scenario -} % 英文タイトル
\ename{○Kenta HARUYAMA (CIT)，Masaki FUJIWARA (CIT)，Ryusei BABA (CIT)，\\
Takumi ISHIGURO (CIT)，Ryuichi UEDA (CIT)，Yasuo HAYASHIBARA (CIT)}	%著者名（英）

\begin{document}
% アブストラクト
\abst{
    This manuscript describes a method for preparing a manuscript for the annual conference of the SICE SI division.
    This manuscript describes a method for preparing a manuscript for the annual conference of the SICE SI division.
    This manuscript describes a method for preparing a manuscript for the annual conference of the SICE SI division.
    This manuscript describes a method for preparing a manuscript for the annual conference of the SICE SI division.
}

% タイトルの出力
\maketitle

% 本文
\section{緒言}

本研究グループでは，end-to-end 学習により，カメラ画像と目標とする
経路の情報を入力として，経路を追従する行動をオンラインで模倣する手
法を提案してきた\cite{haruyama2022}\cite{fujiwara2023}．
また，実験により手法の有効性を検証してきた．
\par
これまで提案した手法では，
LiDARやオドメトリ,事前に作成した幾何学的な地図を入力とする
ROS Navigation Stack\cite{ros-navigation}などの地図に基づいたルールベース制御器によって
経路追従する．
その際，この行動をカメラ画像と目標とする進行方向の情報（以後，目標方向と呼ぶ）
を入力としてオンラインで模倣するように学習する．
これにより，Fig.\ref{fig:camera_base}に示すカメラ画像と目標方向に基づき，
経路を選択して追従する行動を獲得できる．
\par
この手法により，地図に基づく経路追従とカメラ画像を入力とする経路追従の
2つのナビゲーション手段が得られる．この2つの手段を状況
に応じて高い信頼性が見込まれる方を選択することで，
経路追従を継続できる可能性が高まる．
\par
前報では\cite{haruyama2022}\cite{fujiwara2023}，
カメラ画像を入力とする経路追従における，目標方向の出力方法を議論の対象としておらず，
目標方向の出力を地図に基づいたルールベース制御器に依存していた．
そのため，カメラ画像を入力とする経路追従においても，
事前に作成した幾何学的な地図を必要とする問題があった．
この問題の解決には，地図に基づいたルールベース制御器に依存しない，
つまり幾何学的な地図を必要としない目標方向の出力方法が必要だと考えられる．
解決に向けて，幾何学的な地図を用いずに，目標方向を出力する手段を検討する．
% \cite{shimada2020}
% % [島田〜原]
% との統合を行う．
% これにより，目標方向の生成，経路追従をカメラ画像のみで行い，
% 設定された経路に従って自律移動することが期待される．
\par
本稿では，カメラ画像を入力とする経路追従に対して，
島田らが提案した「突き当りまで」という「条件」や「直進」などの「行動」
による経路の表現（以後，シナリオと呼ぶ）\cite{shimada2020}を
目標方向へ変換し，出力する仕組みを追加する．
そして，事前に作成した幾何学的な地図を用いず，カメラ画像とシナリオに
基づいて経路を追従し，目的地まで自律移動する手法を提案する．
また，実ロボットを用いた実験を通して，提案した手法の有効性を検証する．
% \begin{figure}[htbp]
%     \centering
%      \includegraphics[height=40mm,width=80mm]{./figs/map_base.png}
%      \caption{Imitation learning of path-tracking}\label{fig:mapbase}
% \end{figure}
\begin{figure}[h!]
    \centering
     \includegraphics[height=40mm,width=80mm]{./figs/camera_base.png}
     \caption{Path selection and following behavior based on camera images and
     target direction by imitation learning.}\label{fig:camera_base}
\end{figure}
% \section{カメラ画像を入力とする経路追従}

\begin{figure*}[t]
    \centering
     \includegraphics[height=60mm,width=160mm]{./figs/abs50.png}
     \caption{Overview of the system used in the method}\label{fig:system}
\end{figure*}
\section{提案手法}
提案する手法では，カメラ画像と「三叉路まで直進」といったシナリオを入力として，
経路を追従することで目的地まで自律移動する．
この手法の特徴として，目的地までの自律移動に，LiDARなどを用いて事前に
作成した幾何学的な地図を必要としないことが挙げられる．
ここでは全体の流れについて述べた後，それぞれの詳細を述べる．
\par
手法のシステム概要と動作例をFig.\ref{fig:system}に示す．
システムは，ロボットに取り付けたカメラから得た画像データと，
人間が作成したシナリオを入力とし，
\begin{enumerate}
    \item [1）] シナリオの「条件」の判定に用いる，通路の特徴を分類する通路分類器
    \item [2）] シナリオを解釈し，目標方向へ変換して出力するモジュール
    \item [3）] カメラ画像と目標方向に基づく経路追従モジュール\
\end{enumerate}
% 1）シナリオの「条件」の判定に用いる，通路の特徴を分類する通路分類器\\
% 2）シナリオを解釈し，目標方向へ変換して出力するモジュール\\
% 3）カメラ画像と目標方向に基づく経路追従モジュール\\
の3つのモジュールで構成されている．
Fig.\ref{fig:system}の動作例では，人間によって作成された「三叉路まで直進．右折」
というシナリオがシステムへ入力されている．
この入力されたシナリオを2）のモジュールが「条件」と「行動」に分解し，
この行動を目標方向へ変換して出力する．
具体的には，1）の通路分類器から「三叉路」という分類結果が得られるまで，
「直進」の目標方向を出力し，3）の経路追従モジュールへ与える．
3）の経路追従モジュールはカメラ画像と与えられた目標方向をもとに，
「直進」の経路を追従する．
1）の通路分類器から「三叉路」の分類結果が得られた場合，
2）のモジュールは次に「右折」の目標方向を出力し，
3）のモジュールは「右折」の経路を追従する．



\subsection{カメラ画像と目標方向に基づく経路追従}
カメラ画像と目標方向に基づいて，
経路を追従するモジュールについて述べる．
このモジュールは，地図を用いたルールベース制御器の出力を模倣するように学習して,
カメラ画像に基づいた経路追従行動を獲得する．
学習後，外界センサをRGBカメラのみとして，経路追従が可能という特長がある．
\par
Fig.\ref{fig:learning}に経路追従モジュールのシステムを示す．
学習時は，2D-LiDARやオドメトリ，事前に作成した幾何学的な地図に
基づいたルールベース制御器（ROS Navigation Stack\cite{ros-navigation}）によって，設定した経路を走行する．
その際，入力をカメラ画像，目標方向，
出力をヨー方向の角速度（地図を用いたルールベース制御器が出力）として，0.2秒周期でデータセットに加える．
さらに，バッチサイズを８として教師データをデータセットから抽出し，
0.2秒の周期でオンラインで学習する．
このデータセットにデータを追加，データの抽出，学習の1連の流れを1ステップとする．
ここでの目標方向は地図に基づいたルールベースの制御器から出力される信号を用いる．
データセットの収集には，藤原ら\cite{fujiwara2023}が提案した
% \cite{fujiwara2023}
% [藤原]
データセットの不均衡の改善，
学習時における積極的な蛇行といった最も成功率の高い手法を用いる．\par

学習後，カメラ画像と
後述するシナリオを解釈し，変換するモジュールから目標方向を与え，
出力されるヨー方向の角速度によりロボットを制御する．
% \cite{haruyama2022}\cite{fujiwara2023}では
% [春山][藤原]
% 前報では
% 学習後も，
% 地図に基づいたルールベース制御器から生成される目標方向を用いていた．
% 本稿では，学習後で用いる目標方向を，
% 次の節で述べるシナリオに基づくナビゲーションから生成する．
% なお学習器のネットワークは
% \cite{haruyama2022}\cite{fujiwara2023}
% % [春山][藤原]
% と同様の構成を用いる．
\begin{figure}[h!]
    \centering
     \includegraphics[height=45mm,width=80mm]{./figs/learning_gamma.png}
     \caption{Imitation learning system with target direction}\label{fig:learning}
\end{figure}
\subsection{シナリオから目標方向への変換}
目的地まで到達するために必要な目標方向を出力するモジュールについて述べる．
このモジュールは入力されたシナリオから「突き当りまで」という「条件」や
「直進」などの「行動」を解釈し，単語で構成された経路を目標方向へ変換して出力する．
後述する分類器と組み合わせることで，幾何学的な地図を用いずに，
目的地までの自律移動に必要な目標方向を，経路追従モジュールへ与えることを可能とする．
% このナビゲーションは\cite{shimada2020}
% [島田ら]
% で提案された人の道案内情報を収集・分析し，
% そのデータを基にトポロジカルマップの形式とシナリオ（経路の表現）を用いて
% ロボットをナビゲーションする手法をベースに構成する．
% ~にトポロジカルマップとシナリオの例を示す．
\par
% 目標方向は\cite{shimada2020}
% % [島田]
% で開発された
% 1）シナリオからロボットの制御用の手順を生成する機能を拡張して生成する．
「3つ目の三叉路まで直進．右折．突き当たりまで直進．停止」
というシナリオを目標方向として変換する例を述べる．
このモジュールでは，シナリオを句点ごとに分解後，
「条件」と「行動」を示す言葉を抽出し，以下の項目に分けて登録する．\\
1）通路の特徴\\
2）順番\\
3）方向\\
4）行動\\
シナリオの例は句点ごとに
3つ目の三叉路まで直進/　
右折/　
突き当たりまで直進/　
停止/　
と区切られる．
1つ目の区切りでは\\
1）通路の特徴：三叉路\\
2）順番：3\\
4）行動：直進\\
2つ目の区切りでは\\
4）行動：右折\\
が登録される．
この一連の作業を末尾の区切りである「停止」が登録されるまで行う．
ここで登録される「行動」をTable \ref{tab:target}に示すワンホットベクトルで表現し，
目標方向として，前述の経路追従モジュールへ与える．

\begin{table}[]
    \centering
    \caption{Target direction and data for imitation learning}\label{tab:target}
    \begin{tabular}{|c|c|}
    \hline
    Target direction & Data        \\
    \hline
    Go straight   & {[}1，0，0{]} \\
    Turn left   & {[}0，1，0{]} \\
    Turn right   & {[}0，0，1{]} \\
    Stop   & {[}0，0，0{]}\\
    \hline
    \end{tabular}
    \end{table}

% \par
% 島田らは1）の機能の他に2）通路の特徴を検出する機能
% 3）経路に沿って通路を走行する2つの機能を開発している．
% しかし，\cite{shimada2020}\cite{hara2022}
% % [島田][原]
% の手法では2）と3）にLiDARや全天球カメラのセンサ入力を
% 必要としている．
% そのため，本稿では．
% 2）の通路の特徴検出を，次の小節で述べるカメラ画像による手法，
% 3）に関しては前述した経路選択機能をもつ学習器へ変更している．

\subsection{通路の特徴を分類する通路分類器}
シナリオで用いる「条件」の達成を判別するために必要な通路の特徴を
カメラ画像と深層学習によって分類する通路分類器について述べる．
このモジュールは通路の特徴をRGBカメラのみで検出可能という
特長を持っている．
通路分類器の概要をFig.\ref{fig:lrcn}に示す．通路分類器は
フレーム数16の一連の連続した画像データ（画像サイズは64×48）
を入力とし，通路の特徴の分類を出力する．
通路の特徴の分類は島田らの手法\cite{shimada2020}に倣い，
Fig.\ref{fig:intersection}に示した８つとする．
通路分類器のネットワークアーキテクチャは
Dhaivatらが提案するCNNとLSTMを組み合わせたLRCN\cite{lrcn}を参考として構築した．
なおシステムでは，CNNアーキテクチャにMobileNetV3-Large\cite{v3}を用いる．
% シーケンスデータのフレーム数は16 

\begin{figure}[h!]
    \centering
     \includegraphics[height=40mm,width=80mm]{./figs/LRCN_gai.png}
     \caption{Types of corridor features classifier overview}\label{fig:lrcn}
\end{figure}
\begin{figure}[h!]
    \centering
     \includegraphics[height=40mm,width=80mm]{./figs/type.png}
     \caption{Types of corridor features}\label{fig:intersection}
\end{figure}
\par
次に分類器のデータセットの作成について述べる．
地図に基づいたルールベース制御器によって経路を走行し，
その際，フレーム数16の一連の連続したカメラ画像と
通路の分類ラベルを1組とし，0.125秒周期でデータセットへ加える．
その際，分類ラベルのアノテーションは，地図上で対応する分類ラベルを生成する機能を
追加した地図に基づいたルールベース制御器の出力を用いて，自動的に行う．
\par
学習に用いるデータセット内で，各クラスのデータ数が大きく異なる不均衡データは，
分類に大きな影響を与える
\cite{hukin}
とされている．
そのため，本稿では学習する際に，データセット内のクラス間のデータ数によって
重み付けを行うコストアプローチ\cite{cost}を導入している．

\section{実験}
実ロボットを用いて，
提案するカメラ画像とシナリオに基づく経路追従手法により，
ロボットが目的地へ到達可能であるか検証する．

\subsection{実験装置}
実験にはFig.\ref{fig:gamma}で示すカメラを3つ搭載したロボットを使用する．
\begin{figure}[htbp]
    \centering
     \includegraphics[height=40mm]{./figs/gamma_sensor.png}
     \caption{Experimental setup}\label{fig:gamma}
\end{figure}
\subsection{実験方法}
実験環境としてFig.\ref{fig:cit3f}で示した千葉工業大学津田沼キャンパス2号館3階の廊下を用いる．
% [藤原]と比較すると，突き当りが追加され，
% CとDが2つのこと行動を取ることが可能な分岐路へ変化している．
% 経路[藤原]で用いたa~fに図=で示したc~を追加した，a~nの順で走行する
% 前報と比較すると，突き当り，2つの行動がとれる分岐路C，Dが追加されている．
経路はFig.\ref{fig:newroute}で示したaからnを順番に走行する．\par
実験では島田らが用いた50例のシナリオの中から，
Fig.\ref{fig:cit3f}に示したエリアを対象として抽出した7例を用いる．
シナリオの抽出において，1．地図を用いたルールベース制御器で通行が困難な場所が含まれるもの．
2．その場で「右を向く」といった経路追従モジュールが達成困難なもの
を除外している．\par
まずはじめに通路分類器の訓練を行う．
前述の経路を地図に基づいたルールベース制御器の出力を用いて，3周し，
データセットを収集する．
% その際，データセットへのデータの追加は0.125秒周期で行う．
収集したデータは，1，2周目を訓練データとし，3周目をテストデータとする．
それぞれのデータセットを構成する一連の連続した画像データの組数は
訓練データでは5781組，テストデータでは2902組である．
訓練はバッチサイズを32として，30epoch行った．
訓練の結果，テストデータに対するAccuracyは0.98となった．\par
次に経路追従モジュールの訓練を行う．
通路分類器の訓練と同様の経路を，オンラインで模倣学習しながら1周走行する．
その際のステップ数は12000となった．
% データセットへのデータの追加は0.2秒周期で行う．
\par
2つのモジュールを訓練後，抽出した7例のうちの1例のシナリオをシステムに入力する．
この入力したシナリオのスタート地点へシナリオに基づいた向きでロボットを配置し，実験を開始する．
なお，経路から外れるといった要因で走行の継続が困難になった場合でも即時失敗とせず，
失敗箇所を記録しながら，人間が介入し，実験を継続する．
Fig.\ref{fig:scenario_exp}に実験で用いた7例のシナリオを示す．
% 今回は，実験環境を2号館3階の一部のエリアに限定しているが，
% 今後はフロア全体へ拡張する予定である．
\begin{figure}[htbp]
    \centering
     \includegraphics[height=40mm,width=80mm]{./figs/cit3f.png}
     \caption{Experimental environment}\label{fig:cit3f}
\end{figure}
% \begin{figure}[htbp]
%     \centering
%      \includegraphics[height=100mm,width=80mm]{./figs/newroute.png}
%      \caption{Route for the added experiment}\label{fig:newroute}
% \end{figure}

% \begin{figure}[h!]
%     \centering
%      \includegraphics[height=50mm]{./figs/scenario24.png}
%      \caption{Example of the scenario}\label{fig:scenario24}
% \end{figure}
\begin{figure}[htbp]\vspace*{-2zh}
    \centering
     \includegraphics[height=120mm,width=80mm]{./figs/route.png}
     \caption{Route for the experiment}\label{fig:newroute}
\end{figure}

\subsection{実験結果}
% 実験結果をTable \ref{tab:result}に示す．
% 表はそれぞれ実験に用いたシナリオの番号，学習器の出力が要因による介入の回数，
% 通路分類の間違いを要因とする介入の回数である．
Fig.\ref{fig:exp_path}にFig.9(f)における経路追従の様子を示す．
結果として，抽出した7例のうち，すべての例でロボットが人間の介入なしで，
指定された経路を追従して目的地へ到達した．
以上の結果から，提案するカメラ画像とシナリオに基づいて，
経路を追従して目的地まで自律移動する手法の有効性が確認された．
% シナリオに基づくナビゲーションとカメラ画像による通路分類を追加したシステムが，
% 適切に動作することを確認した．


\section{結言}
本稿では，事前に作成した幾何学的な地図を用いず，カメラ画像とシナリオに基づいて
経路を追従して目的地まで自律移動する手法を提案した．
そして，実ロボットを用いた実験により提案手法の有効性を検証した．
% 経路選択機能を持つ学習器を用いた走行に対して，
% カメラ画像からの目標方向の生成を目的として，シナリオを
% 用いたナビゲーションと通路分類器の
% 追加を行った．
% 実験では,提案手法により，指示された経路を
% カメラ画像に基づいて，追従し，目的地へ到達なことを確認した
% 学習器がカメラ画像と生成された目標方向を用いて，
% 指定された経路に沿って目的地へ到達可能なことを確認した．
% 今後は，実験環境をより広い屋内，または屋外へと拡大する予定である．
% \begin{figure}[htbp]
%     \centering
%      \includegraphics[height=40mm,width=80mm]{./figs/keiro.png}
%      \caption{Experimental environment}\label{fig:cit3f}
% \end{figure}

\printbibliography[title=参考文献]
\setlength\textfloatsep{0pt}
\begin{figure*}[t]\vspace*{-2zh}
    \begin{tabular}{ccc}
        \begin{minipage}[t]{0.5\textwidth}
            \centering
            \includegraphics[keepaspectratio, width=80mm]{figs/scenario01.png}
            \subcaption{Scenario 01}
            \label{composite}
        \end{minipage} &
        \begin{minipage}[t]{0.5\textwidth}
            \centering
            \includegraphics[keepaspectratio, width=80mm]{figs/scenario05.png}
            \subcaption{Scenario 02}
            \label{Gradation}
        \end{minipage} \\
        \begin{minipage}[t]{0.5\textwidth}
            \centering
            \includegraphics[keepaspectratio, width=80mm]{figs/scenario20.png}
            \subcaption{Scenario 03}
            \label{fill}
        \end{minipage} &
        \begin{minipage}[t]{0.5\textwidth}
            \centering
            \includegraphics[keepaspectratio, width=80mm]{figs/scenario21.png}
            \subcaption{Scenario 04}
            \label{transform}
        \end{minipage} \\
        \begin{minipage}[t]{0.5\textwidth}
            \centering
            \includegraphics[keepaspectratio, width=80mm]{figs/scenario22.png}
            \subcaption{Scenario 05}
            \label{image1}
        \end{minipage} &
        \begin{minipage}[t]{0.5\textwidth}
            \centering
            \includegraphics[keepaspectratio, width=80mm]{figs/scenario24.png}
            \subcaption{Scenario 06}
            \label{fig:scenario24}
        \end{minipage}\\
        \begin{minipage}[t]{0.5\textwidth}
            \centering
            \includegraphics[keepaspectratio, width=80mm]{figs/scenario50.png}
            \subcaption{Scenario 07}
            \label{imagess}
        \end{minipage}
    \end{tabular}
    \caption{Example of the scenario used in the experiment}\label{fig:scenario_exp}
\end{figure*}
\begin{figure*}[t]\vspace*{-10zh}
    \begin{tabular}{ccc}
        \begin{minipage}[t]{0.5\textwidth}
            \centering
            \includegraphics[keepaspectratio, width=80mm]{figs/exp_path_follow_0.png}
            \subcaption{First 3-way：Go straight}
        \end{minipage} &
        \begin{minipage}[t]{0.5\textwidth}
            \centering
            \includegraphics[keepaspectratio, width=80mm]{figs/exp_path_follow_1.png}
            \subcaption{Second 3-way：Go straight}
           
        \end{minipage} \\
        \begin{minipage}[t]{0.5\textwidth}
            \centering
            \includegraphics[keepaspectratio, width=80mm]{figs/exp_path_follow_2.png}
            \subcaption{Third 3-way：Turn right}
            
        \end{minipage} &
        \begin{minipage}[t]{0.5\textwidth}
            \centering
            \includegraphics[keepaspectratio, width=80mm]{figs/exp_path_follow_4.png}
            \subcaption{End：Turn left}
           
        \end{minipage} \\
        \begin{minipage}[t]{0.5\textwidth}
            \centering
            \includegraphics[keepaspectratio, width=80mm]{figs/exp_path_follow_5.png}
            \subcaption{End：Stop}
        \end{minipage}
    \end{tabular}

    \caption{Behavior of path following in scenario}\label{fig:exp_path}
\end{figure*}
% \begin{figure*}[htbp]\vspace*{-10zh}
%     \centering
%      \includegraphics[height=60mm,width=180mm]{./figs/exp_etc.png}
%      \caption{Behavior of path following in scenario24}\label{fig:exp}
% \end{figure*}
% \begin{table}[h!]
%     \centering
%     \caption{The number of assistances in the experiment}\label{tab:result}
%     % \begin{tabular}{ccclll}
        
%     % \begin{tabularx}{\textwidth}{X|X|X}
%     \begin{tabularx}{80mm}{|C|C|C|}
%     \hline
%     Scenario number used in the experiment & 
%     Number of assistances for deviating from the path & 
%     Number of assistances due to corridor classification failures \\
%     \hline
%     1       & 0         & 0             \\
%     5       & 0         & 0             \\
%     20      & 0         & 0             \\
%     21      & 0         & 0             \\
%     22      & 0         & 0             \\
%     24      & 0         & 0             \\
%     50      & 0         & 0             \\
%     \hline
%     % \end{tabular}
%     \end{tabularx}
%     \end{table}
\end{document}
